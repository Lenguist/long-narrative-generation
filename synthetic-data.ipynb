{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUT YOUR API KEY HERE\n",
    "OPENAI_API_KEY = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "414"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seed data for synthetic data generation\n",
    "\n",
    "# Reading in data\n",
    "import pandas as pd\n",
    "csv_file_path = 'data/coherence/coherence_human_data.csv'\n",
    "# Read the CSV file back into a DataFrame\n",
    "coherence_human_data_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Combine the 'story1' and 'story2' columns\n",
    "combined_stories = coherence_human_data_df['story1'].tolist() + coherence_human_data_df['story2'].tolist()\n",
    "\n",
    "# Get unique stories\n",
    "unique_stories = list(set(combined_stories))\n",
    "len(unique_stories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherence Errorifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_text = \"\"\"\n",
    "In the heart of the bustling city, there lived a man named Alex, whose days were a monotonous blend of work and solitude. The city’s vibrant lights couldn't illuminate the emptiness he felt inside. Love, a distant dream, flickered in his heart like a fading star. Alex longed for someone to share his hopes, dreams, and quiet dinners.\n",
    "\n",
    "Each evening, Alex strolled through the city park, hoping to find a connection amidst the sea of strangers. The park, a canvas of life, was where laughter danced in the air, and hearts spoke without words. Yet, Alex walked alone, his gaze often lingering on happy couples, wondering when his moment would come.\n",
    "\n",
    "One autumn evening, as amber leaves rustled underfoot, Alex’s eyes met Emma’s. She was sitting alone on a bench, lost in a book, her laughter echoing with the lines she read. Intrigued, Alex mustered his courage and approached her, asking about the book that brought such joy.\n",
    "\n",
    "Their conversation flowed effortlessly, like a melody long rehearsed yet spontaneously played. Emma, with her warm smile and insightful eyes, saw through Alex’s guarded exterior. She listened to his dreams and shared her own, painting her world with words that resonated with his soul.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alex longed for someone to share his hopes, dreams, and quiet dinners. Each evening, Alex strolled through the city park, hoping to find a connection amidst the sea of strangers. \n",
      "In the heart of the bustling city, there lived a man named Alex, whose days were a monotonous blend of work and solitude. She was sitting alone on a bench, lost in a book, her laughter echoing with the lines she read. Intrigued, Alex mustered his courage and approached her, asking about the book that brought such joy. Yet, Alex walked alone, his gaze often lingering on happy couples, wondering when his moment would come. The park, a canvas of life, was where laughter danced in the air, and hearts spoke without words. The city’s vibrant lights couldn't illuminate the emptiness he felt inside. One autumn evening, as amber leaves rustled underfoot, Alex’s eyes met Emma’s. Their conversation flowed effortlessly, like a melody long rehearsed yet spontaneously played. Emma, with her warm smile and insightful eyes, saw through Alex’s guarded exterior. She listened to his dreams and shared her own, painting her world with words that resonated with his soul.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mbondarenko/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# coherence_errorifier - takes in a good text (either human or AI generated)\n",
    "# removes random sentences (with a prob of p_remove for each sentences)\n",
    "# reshuffles random sentences (with a prob p_reshuffle for any given sentences, sentence to shuffle with is picked randomly)\n",
    "# adds random sentences (to be implemented later)\n",
    "# each of those should introduce a coherence error\n",
    "\n",
    "import random\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def coherence_errorifier(text, p_remove, p_reshuffle, p_group_reshuffle, max_group_size=2):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Remove random sentences\n",
    "    sentences = [s for s in sentences if random.random() > p_remove]\n",
    "\n",
    "    # Reshuffle individual sentences\n",
    "    for i in range(len(sentences)):\n",
    "        if random.random() < p_reshuffle:\n",
    "            swap_index = random.randint(0, len(sentences) - 1)\n",
    "            sentences[i], sentences[swap_index] = sentences[swap_index], sentences[i]\n",
    "\n",
    "    # Reshuffle groups of sentences\n",
    "    i = 0\n",
    "    group_size = 0\n",
    "    while i < len(sentences):\n",
    "        if random.random() < p_group_reshuffle:\n",
    "            group_size = random.randint(1, min(max_group_size, len(sentences) - i))\n",
    "            swap_index = random.randint(0, len(sentences) - group_size)\n",
    "            # Swap groups of sentences\n",
    "            for j in range(group_size):\n",
    "                if i + j < len(sentences) and swap_index + j < len(sentences):\n",
    "                    sentences[i + j], sentences[swap_index + j] = sentences[swap_index + j], sentences[i + j]\n",
    "\n",
    "        i += group_size\n",
    "\n",
    "    # Add random sentences (to be implemented)\n",
    "\n",
    "    # Reconstruct the text\n",
    "    modified_text = ' '.join(sentences)\n",
    "    return modified_text\n",
    "\n",
    "# Example usage\n",
    "text = init_text\n",
    "modified_text = coherence_errorifier(text, p_remove=0.05, p_reshuffle=0.1, p_group_reshuffle=0.1, max_group_size=3)\n",
    "print(modified_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:03<00:00, 1428.18it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "errorified_data = []\n",
    "with tqdm(total=5000) as pbar:\n",
    "    while len(errorified_data) < 5000:\n",
    "        for text in combined_stories:\n",
    "            # Create a new datapoint dictionary in each iteration\n",
    "            datapoint = {}\n",
    "            modified_text = coherence_errorifier(text, p_remove=0.05, p_reshuffle=0.1, p_group_reshuffle=0.1, max_group_size=3)\n",
    "            datapoint[\"errorified\"] = modified_text\n",
    "            datapoint[\"original\"] = text\n",
    "            errorified_data.append(datapoint)\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "            # Break if we reached the desired length\n",
    "            if len(errorified_data) >= 5000:\n",
    "                break\n",
    "\n",
    "# Ensure we don't exceed the length of 5000\n",
    "errorified_data = errorified_data[:5000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "data = pd.DataFrame(errorified_data)\n",
    "\n",
    "# Format One\n",
    "preference_format = data.apply(lambda x: pd.Series({\n",
    "    \"story1\": x['original'] if random.choice([True, False]) else x['errorified'],\n",
    "    \"story2\": x['errorified'] if x['original'] in x else x['original'],\n",
    "    \"preference\": 0 if x['original'] in x else 1\n",
    "}), axis=1)\n",
    "\n",
    "# Format Two\n",
    "classifier_format = pd.concat([\n",
    "    pd.DataFrame({\"story\": data[\"original\"], \"label\": 1}),\n",
    "    pd.DataFrame({\"story\": data[\"errorified\"], \"label\": 0})\n",
    "]).reset_index(drop=True)\n",
    "\n",
    "# Saving to files (adjust the file paths as needed)\n",
    "preference_format.to_csv(\"synthetic_preference_coherence.csv\", index=False)\n",
    "classifier_format.to_csv(\"synthetic_classifier_coherence.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance errorifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevance_erorifier - takes in a good text (either human or AI generated)\n",
    "# finds sentences closest in embedding to the prompt, removes them (remove N closest sentences)\n",
    "# adds N random sentences from other texts\n",
    "# each of those should introduce a relevance error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "  api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "\n",
    "def get_gpt3_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input = [text], model=model).data[0].embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Text 1. This is a sample sentence. Another example sentence.\n",
      "Relevance Errorified: Text 2. More sample content.. More sample content.. Different example sentence. More sample content.\n",
      "Premise: Premise 1\n",
      "\n",
      "Original: Text 2. Different example sentence. More sample content.\n",
      "Relevance Errorified: Another example sentence.. Another example sentence.. Another example sentence.. This is a sample sentence. Another example sentence.\n",
      "Premise: Premise 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def relevance_errorifier(text_premise_pairs, get_embedding, p_remove=0.1, N=2):\n",
    "    \"\"\"\n",
    "    Modify each text in a list of [text, premise] pairs to introduce relevance errors.\n",
    "    Returns a list of lists containing the original text, modified text, and premise.\n",
    "\n",
    "    :param text_premise_pairs: List of [text, premise] pairs.\n",
    "    :param get_embedding: Function to get the embedding of a text.\n",
    "    :param p_remove: Probability of removing a sentence from the original texts.\n",
    "    :param N: Number of closest sentences to the premise to remove.\n",
    "    :return: List of [original text, modified text, premise] lists.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for i, (text, premise) in enumerate(text_premise_pairs):\n",
    "        try:\n",
    "            # Use other texts as the pool for other-texts\n",
    "            other_texts = [pair[0] for j, pair in enumerate(text_premise_pairs) if j != i]\n",
    "\n",
    "            # Split the current text into sentences\n",
    "            sentences = text.split('. ')  # Basic split, can be improved with NLP tools\n",
    "\n",
    "            # Find and remove N sentences closest to the premise\n",
    "            premise_embedding = get_embedding(premise)\n",
    "            sentence_embeddings = [get_embedding(s) for s in sentences]\n",
    "            similarities = [cosine_similarity([e], [premise_embedding])[0][0] for e in sentence_embeddings]\n",
    "            closest_sentences_idx = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:N]\n",
    "            sentences_with_removal = [s for j, s in enumerate(sentences) if j not in closest_sentences_idx]\n",
    "\n",
    "            # Add random sentences from other texts\n",
    "            random_sentences = []\n",
    "            for _ in range(N):  # Adding as many sentences as we removed\n",
    "                random_text = random.choice(other_texts)\n",
    "                random_sentences.extend(random_text.split('. '))  # Basic split\n",
    "\n",
    "            sentences_with_removal.extend(random.sample(random_sentences, N))\n",
    "\n",
    "            # Reconstruct the text\n",
    "            modified_text = '. '.join(sentences_with_removal)\n",
    "\n",
    "            # Append the result\n",
    "            results.append([text, modified_text, premise])\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during processing: {e}\")\n",
    "            results.append([text, '', premise])  # Append with empty modified text in case of error\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "text_premise_pairs = [\n",
    "    [\"Text 1. This is a sample sentence. Another example sentence.\", \"Premise 1\"],\n",
    "    [\"Text 2. Different example sentence. More sample content.\", \"Premise 2\"]\n",
    "]\n",
    "\n",
    "results = relevance_errorifier(text_premise_pairs, get_gpt3_embedding, p_remove=0.05, N=5)\n",
    "for result in results:\n",
    "    print(\"Original:\", result[0])\n",
    "    print(\"Relevance Errorified:\", result[1])\n",
    "    print(\"Premise:\", result[2])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "414"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file_path = 'data/coherence/coherence_human_data.csv'\n",
    "coherence_human_data_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Create a list of tuples (story, premise) for both 'story1' and 'story2'\n",
    "story_premise_pairs = [(row['story1'], row['premise']) for _, row in coherence_human_data_df.iterrows()] + \\\n",
    "                      [(row['story2'], row['premise']) for _, row in coherence_human_data_df.iterrows()]\n",
    "\n",
    "# Remove duplicate pairs\n",
    "unique_story_premise_pairs = list(set(story_premise_pairs))\n",
    "\n",
    "len(unique_story_premise_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred during processing: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "An error occurred during processing: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "An error occurred during processing: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "An error occurred during processing: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "An error occurred during processing: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "An error occurred during processing: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "An error occurred during processing: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "An error occurred during processing: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "An error occurred during processing: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    }
   ],
   "source": [
    "results = relevance_errorifier(unique_story_premise_pairs, get_gpt3_embedding, p_remove=0.05, N=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_erorrified_df = pd.DataFrame(results)\n",
    "relevance_erorrified_df.rename(columns={0: 'original', 1: 'errorified', 2: 'premise'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>errorified</th>\n",
       "      <th>premise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anna Cooper entered the office, a middle-aged ...</td>\n",
       "      <td>She wore a white uniform shirt and black pants...</td>\n",
       "      <td>Anna Cooper begins her job as a janitor at a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;br&gt;&lt;br&gt;Aimee Kincaid stepped off the train an...</td>\n",
       "      <td>The bustle of busy commuters and pedestrians w...</td>\n",
       "      <td>Aimee Kincaid goes home after a long day at wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>end the way she feared hers might end, but cou...</td>\n",
       "      <td>end the way she feared hers might end, but cou...</td>\n",
       "      <td>Kyle comforts Aimee and the two share a heart-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;br&gt;&lt;br&gt;Kyle looked at her with pity in his ey...</td>\n",
       "      <td>&lt;br&gt;&lt;br&gt;Kyle looked at her with pity in his ey...</td>\n",
       "      <td>Kyle comforts Aimee and the two share a heart-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brad Barton sat on the sofa in the living-room...</td>\n",
       "      <td>Brad Barton sat on the sofa in the living-room...</td>\n",
       "      <td>Shannon and Brad Barton's marriage is on the r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>Maria Martinez was still running. She could fe...</td>\n",
       "      <td>Maria Martinez was still running. She could fe...</td>\n",
       "      <td>A group of survivors find themselves in a stra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>happened and they started looking for John Doe...</td>\n",
       "      <td>We also have some evidence that points to some...</td>\n",
       "      <td>Valerie makes it to safety and the police catc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>on the table, “I really appreciate you guys in...</td>\n",
       "      <td>on the table, “I really appreciate you guys in...</td>\n",
       "      <td>Kyle comforts Aimee and the two share a heart-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>the murders and offered to investigate. Captai...</td>\n",
       "      <td>the murders and offered to investigate. Kitty ...</td>\n",
       "      <td>Kitty begins her investigation by speaking to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>It was all silent for the first time in a long...</td>\n",
       "      <td>“The first time is the hardest. But it will ge...</td>\n",
       "      <td>A group of survivors find themselves holed up ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>414 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              original  \\\n",
       "0    Anna Cooper entered the office, a middle-aged ...   \n",
       "1    <br><br>Aimee Kincaid stepped off the train an...   \n",
       "2    end the way she feared hers might end, but cou...   \n",
       "3    <br><br>Kyle looked at her with pity in his ey...   \n",
       "4    Brad Barton sat on the sofa in the living-room...   \n",
       "..                                                 ...   \n",
       "409  Maria Martinez was still running. She could fe...   \n",
       "410  happened and they started looking for John Doe...   \n",
       "411  on the table, “I really appreciate you guys in...   \n",
       "412  the murders and offered to investigate. Captai...   \n",
       "413  It was all silent for the first time in a long...   \n",
       "\n",
       "                                            errorified  \\\n",
       "0    She wore a white uniform shirt and black pants...   \n",
       "1    The bustle of busy commuters and pedestrians w...   \n",
       "2    end the way she feared hers might end, but cou...   \n",
       "3    <br><br>Kyle looked at her with pity in his ey...   \n",
       "4    Brad Barton sat on the sofa in the living-room...   \n",
       "..                                                 ...   \n",
       "409  Maria Martinez was still running. She could fe...   \n",
       "410  We also have some evidence that points to some...   \n",
       "411  on the table, “I really appreciate you guys in...   \n",
       "412  the murders and offered to investigate. Kitty ...   \n",
       "413  “The first time is the hardest. But it will ge...   \n",
       "\n",
       "                                               premise  \n",
       "0    Anna Cooper begins her job as a janitor at a s...  \n",
       "1    Aimee Kincaid goes home after a long day at wo...  \n",
       "2    Kyle comforts Aimee and the two share a heart-...  \n",
       "3    Kyle comforts Aimee and the two share a heart-...  \n",
       "4    Shannon and Brad Barton's marriage is on the r...  \n",
       "..                                                 ...  \n",
       "409  A group of survivors find themselves in a stra...  \n",
       "410  Valerie makes it to safety and the police catc...  \n",
       "411  Kyle comforts Aimee and the two share a heart-...  \n",
       "412  Kitty begins her investigation by speaking to ...  \n",
       "413  A group of survivors find themselves holed up ...  \n",
       "\n",
       "[414 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevance_erorrified_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Assuming df is your DataFrame with columns 'original', 'errorified', and 'premise'\n",
    "# df = pd.read_csv('your_file.csv')  # Uncomment and modify if you're reading from a CSV file\n",
    "\n",
    "# Helper function for preference format\n",
    "def create_preference_row(row):\n",
    "    use_original = random.choice([True, False])\n",
    "    story1 = row['original'] if use_original else row['errorified']\n",
    "    story2 = row['errorified'] if use_original else row['original']\n",
    "    preference = 0 if use_original else 1\n",
    "    return pd.Series({\"story1\": story1, \"story2\": story2, \"preference\": preference, \"premise\": row['premise']})\n",
    "\n",
    "# Format One (Preference Format)\n",
    "preference_format = relevance_erorrified_df.apply(create_preference_row, axis=1)\n",
    "\n",
    "# Format Two (Classifier Format)\n",
    "classifier_format = pd.concat([\n",
    "    pd.DataFrame({\"story\": relevance_erorrified_df[\"original\"], \"label\": 1, \"premise\": relevance_erorrified_df[\"premise\"]}),\n",
    "    pd.DataFrame({\"story\": relevance_erorrified_df[\"errorified\"], \"label\": 0, \"premise\": relevance_erorrified_df[\"premise\"]})\n",
    "]).reset_index(drop=True)\n",
    "\n",
    "# Saving to files (adjust the file paths as needed)\n",
    "preference_format.to_csv(\"synthetic_preference_relevance.csv\", index=False)\n",
    "classifier_format.to_csv(\"synthetic_classifier_relevance.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
