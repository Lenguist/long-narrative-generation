{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "414"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seed data for synthetic data generation\n",
    "\n",
    "# Reading in data\n",
    "import pandas as pd\n",
    "csv_file_path = 'data/coherence/coherence_human_data.csv'\n",
    "# Read the CSV file back into a DataFrame\n",
    "coherence_human_data_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Combine the 'story1' and 'story2' columns\n",
    "combined_stories = coherence_human_data_df['story1'].tolist() + coherence_human_data_df['story2'].tolist()\n",
    "\n",
    "# Get unique stories\n",
    "unique_stories = list(set(combined_stories))\n",
    "len(unique_stories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherence Errorifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_text = \"\"\"\n",
    "In the heart of the bustling city, there lived a man named Alex, whose days were a monotonous blend of work and solitude. The city’s vibrant lights couldn't illuminate the emptiness he felt inside. Love, a distant dream, flickered in his heart like a fading star. Alex longed for someone to share his hopes, dreams, and quiet dinners.\n",
    "\n",
    "Each evening, Alex strolled through the city park, hoping to find a connection amidst the sea of strangers. The park, a canvas of life, was where laughter danced in the air, and hearts spoke without words. Yet, Alex walked alone, his gaze often lingering on happy couples, wondering when his moment would come.\n",
    "\n",
    "One autumn evening, as amber leaves rustled underfoot, Alex’s eyes met Emma’s. She was sitting alone on a bench, lost in a book, her laughter echoing with the lines she read. Intrigued, Alex mustered his courage and approached her, asking about the book that brought such joy.\n",
    "\n",
    "Their conversation flowed effortlessly, like a melody long rehearsed yet spontaneously played. Emma, with her warm smile and insightful eyes, saw through Alex’s guarded exterior. She listened to his dreams and shared her own, painting her world with words that resonated with his soul.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alex longed for someone to share his hopes, dreams, and quiet dinners. Each evening, Alex strolled through the city park, hoping to find a connection amidst the sea of strangers. \n",
      "In the heart of the bustling city, there lived a man named Alex, whose days were a monotonous blend of work and solitude. She was sitting alone on a bench, lost in a book, her laughter echoing with the lines she read. Intrigued, Alex mustered his courage and approached her, asking about the book that brought such joy. Yet, Alex walked alone, his gaze often lingering on happy couples, wondering when his moment would come. The park, a canvas of life, was where laughter danced in the air, and hearts spoke without words. The city’s vibrant lights couldn't illuminate the emptiness he felt inside. One autumn evening, as amber leaves rustled underfoot, Alex’s eyes met Emma’s. Their conversation flowed effortlessly, like a melody long rehearsed yet spontaneously played. Emma, with her warm smile and insightful eyes, saw through Alex’s guarded exterior. She listened to his dreams and shared her own, painting her world with words that resonated with his soul.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mbondarenko/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# coherence_errorifier - takes in a good text (either human or AI generated)\n",
    "# removes random sentences (with a prob of p_remove for each sentences)\n",
    "# reshuffles random sentences (with a prob p_reshuffle for any given sentences, sentence to shuffle with is picked randomly)\n",
    "# adds random sentences (to be implemented later)\n",
    "# each of those should introduce a coherence error\n",
    "\n",
    "import random\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def coherence_errorifier(text, p_remove, p_reshuffle, p_group_reshuffle, max_group_size=2):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Remove random sentences\n",
    "    sentences = [s for s in sentences if random.random() > p_remove]\n",
    "\n",
    "    # Reshuffle individual sentences\n",
    "    for i in range(len(sentences)):\n",
    "        if random.random() < p_reshuffle:\n",
    "            swap_index = random.randint(0, len(sentences) - 1)\n",
    "            sentences[i], sentences[swap_index] = sentences[swap_index], sentences[i]\n",
    "\n",
    "    # Reshuffle groups of sentences\n",
    "    i = 0\n",
    "    group_size = 0\n",
    "    while i < len(sentences):\n",
    "        if random.random() < p_group_reshuffle:\n",
    "            group_size = random.randint(1, min(max_group_size, len(sentences) - i))\n",
    "            swap_index = random.randint(0, len(sentences) - group_size)\n",
    "            # Swap groups of sentences\n",
    "            for j in range(group_size):\n",
    "                if i + j < len(sentences) and swap_index + j < len(sentences):\n",
    "                    sentences[i + j], sentences[swap_index + j] = sentences[swap_index + j], sentences[i + j]\n",
    "\n",
    "        i += group_size\n",
    "\n",
    "    # Add random sentences (to be implemented)\n",
    "\n",
    "    # Reconstruct the text\n",
    "    modified_text = ' '.join(sentences)\n",
    "    return modified_text\n",
    "\n",
    "# Example usage\n",
    "text = init_text\n",
    "modified_text = coherence_errorifier(text, p_remove=0.05, p_reshuffle=0.1, p_group_reshuffle=0.1, max_group_size=3)\n",
    "print(modified_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:03<00:00, 1428.18it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "errorified_data = []\n",
    "with tqdm(total=5000) as pbar:\n",
    "    while len(errorified_data) < 5000:\n",
    "        for text in combined_stories:\n",
    "            # Create a new datapoint dictionary in each iteration\n",
    "            datapoint = {}\n",
    "            modified_text = coherence_errorifier(text, p_remove=0.05, p_reshuffle=0.1, p_group_reshuffle=0.1, max_group_size=3)\n",
    "            datapoint[\"errorified\"] = modified_text\n",
    "            datapoint[\"original\"] = text\n",
    "            errorified_data.append(datapoint)\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "            # Break if we reached the desired length\n",
    "            if len(errorified_data) >= 5000:\n",
    "                break\n",
    "\n",
    "# Ensure we don't exceed the length of 5000\n",
    "errorified_data = errorified_data[:5000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "data = pd.DataFrame(errorified_data)\n",
    "\n",
    "# Format One\n",
    "preference_format = data.apply(lambda x: pd.Series({\n",
    "    \"story1\": x['original'] if random.choice([True, False]) else x['errorified'],\n",
    "    \"story2\": x['errorified'] if x['original'] in x else x['original'],\n",
    "    \"preference\": 0 if x['original'] in x else 1\n",
    "}), axis=1)\n",
    "\n",
    "# Format Two\n",
    "classifier_format = pd.concat([\n",
    "    pd.DataFrame({\"story\": data[\"original\"], \"label\": 1}),\n",
    "    pd.DataFrame({\"story\": data[\"errorified\"], \"label\": 0})\n",
    "]).reset_index(drop=True)\n",
    "\n",
    "# Saving to files (adjust the file paths as needed)\n",
    "preference_format.to_csv(\"synthetic_preference_coherence.csv\", index=False)\n",
    "classifier_format.to_csv(\"synthetic_classifier_coherence.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance errorifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevance_erorifier - takes in a good text (either human or AI generated)\n",
    "# removes random sentences (with a prob of p_remove for each sentences)\n",
    "# finds sentences closest in embedding to the prompt, removes them (remove N closest sentences)\n",
    "# adds random sentences from other texts\n",
    "# each of those should introduce a relevance error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
